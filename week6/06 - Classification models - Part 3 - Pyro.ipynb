{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.1"},"colab":{"name":"06 - Classification models - Part 3 - Pyro.ipynb","provenance":[{"file_id":"1BgomTlMmVH83toMtKajKc658W5g6cxvP","timestamp":1583741967446},{"file_id":"1ZYHgP2jyDLYd4ZFdiHUX1HVSdzVTIpu7","timestamp":1582409673354}],"collapsed_sections":[]}},"cells":[{"cell_type":"markdown","metadata":{"id":"Gp-vbU6Lh7rM"},"source":["# Week 6 - Classification models  \n","\n","## Part 3: Travel mode choice - Probit regression"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":336},"id":"K933EZb2hlOK","executionInfo":{"status":"ok","timestamp":1615302450565,"user_tz":-60,"elapsed":2036,"user":{"displayName":"Filipe Rodrigues","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgANv2Od365vLQDnVjoxw2PUote-kpb5nugTmcQwA=s64","userId":"15636531912642599438"}},"outputId":"67fe6f36-044f-43ad-c9fc-fee2e54cdbe7"},"source":["from IPython.display import HTML\n","HTML('<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/GZWqSVl9GPU\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>')"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/GZWqSVl9GPU\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]},"execution_count":1}]},{"cell_type":"markdown","metadata":{"id":"nrv_mhslhl38"},"source":["In this part we will revisit our real world problem of travel model choice. \n","\n","The first part is very similar to previous notebook for part 2: loading data, preprocessing, train/test split, etc. However, in this part, we will consider a Probit regression model. For the sake of simplicty, lets assume that we are just interested in distinguishing between car vs non-car (binary classification problem).\n","\n","Lets just start running the parts corresponding to imports, data loading, preprocessing, train/test split, etc."]},{"cell_type":"markdown","metadata":{"id":"epmK4pol9tm4"},"source":["Import required libraries:"]},{"cell_type":"code","metadata":{"id":"yBlGmJB_99dN","colab":{"base_uri":"https://localhost:8080/","height":119},"executionInfo":{"status":"ok","timestamp":1582409767010,"user_tz":-60,"elapsed":4203,"user":{"displayName":"Filipe Rodrigues","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDaWEE0j1oWjHLxVkWKYDXRh8NUBFCgGYF0XZOsvA=s64","userId":"15636531912642599438"}},"outputId":"40c07dba-bfaf-4fef-ce29-35c05aaf1af6"},"source":["# Install Pyro, if necessary\n","!pip install pyro-ppl"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: pyro-ppl in /usr/local/lib/python3.6/dist-packages (1.2.1)\n","Requirement already satisfied: pyro-api>=0.1.1 in /usr/local/lib/python3.6/dist-packages (from pyro-ppl) (0.1.1)\n","Requirement already satisfied: numpy>=1.7 in /usr/local/lib/python3.6/dist-packages (from pyro-ppl) (1.17.5)\n","Requirement already satisfied: torch>=1.4.0 in /usr/local/lib/python3.6/dist-packages (from pyro-ppl) (1.4.0)\n","Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from pyro-ppl) (3.1.0)\n","Requirement already satisfied: tqdm>=4.36 in /usr/local/lib/python3.6/dist-packages (from pyro-ppl) (4.43.0)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"F5T2UpBy9tm6"},"source":["import numpy as np\n","import pandas as pd\n","from matplotlib import pyplot as plt\n","from sklearn import linear_model\n","import seaborn as sns\n","import torch\n","\n","import pyro\n","import pyro.distributions as dist\n","from pyro.contrib.autoguide import AutoDiagonalNormal, AutoMultivariateNormal\n","from pyro.infer import MCMC, NUTS, HMC, SVI, Trace_ELBO\n","from pyro.optim import Adam, ClippedAdam\n","\n","# fix random generator seed (for reproducibility of results)\n","np.random.seed(42)\n","\n","# matplotlib style options\n","plt.style.use('ggplot')\n","%matplotlib inline\n","plt.rcParams['figure.figsize'] = (16, 10)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"MZHLZly89tnA"},"source":["Load data:"]},{"cell_type":"code","metadata":{"id":"lqArUOVB9tnB","colab":{"base_uri":"https://localhost:8080/","height":241},"executionInfo":{"status":"ok","timestamp":1582409799123,"user_tz":-60,"elapsed":1518,"user":{"displayName":"Filipe Rodrigues","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDaWEE0j1oWjHLxVkWKYDXRh8NUBFCgGYF0XZOsvA=s64","userId":"15636531912642599438"}},"outputId":"9151a600-5f6d-470e-c332-4a3d11b3bc31"},"source":["# load csv\n","df = pd.read_csv(\"http://mlsm.man.dtu.dk/mbml/modechoice_data.csv\")\n","df.head()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Unnamed: 0</th>\n","      <th>individual</th>\n","      <th>hinc</th>\n","      <th>psize</th>\n","      <th>ttme_air</th>\n","      <th>invc_air</th>\n","      <th>invt_air</th>\n","      <th>gc_air</th>\n","      <th>ttme_train</th>\n","      <th>invc_train</th>\n","      <th>invt_train</th>\n","      <th>gc_train</th>\n","      <th>ttme_bus</th>\n","      <th>invc_bus</th>\n","      <th>invt_bus</th>\n","      <th>gc_bus</th>\n","      <th>invc_car</th>\n","      <th>invt_car</th>\n","      <th>gc_car</th>\n","      <th>mode_chosen</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td>70.0</td>\n","      <td>30.0</td>\n","      <td>4.0</td>\n","      <td>10.0</td>\n","      <td>61.0</td>\n","      <td>80.0</td>\n","      <td>73.0</td>\n","      <td>44.0</td>\n","      <td>24.0</td>\n","      <td>350.0</td>\n","      <td>77.0</td>\n","      <td>53.0</td>\n","      <td>19.0</td>\n","      <td>395.0</td>\n","      <td>79.0</td>\n","      <td>4.0</td>\n","      <td>314.0</td>\n","      <td>52.0</td>\n","      <td>1.0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1</td>\n","      <td>8.0</td>\n","      <td>15.0</td>\n","      <td>4.0</td>\n","      <td>64.0</td>\n","      <td>48.0</td>\n","      <td>154.0</td>\n","      <td>71.0</td>\n","      <td>55.0</td>\n","      <td>25.0</td>\n","      <td>360.0</td>\n","      <td>80.0</td>\n","      <td>53.0</td>\n","      <td>14.0</td>\n","      <td>462.0</td>\n","      <td>84.0</td>\n","      <td>4.0</td>\n","      <td>351.0</td>\n","      <td>57.0</td>\n","      <td>2.0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>2</td>\n","      <td>62.0</td>\n","      <td>35.0</td>\n","      <td>2.0</td>\n","      <td>64.0</td>\n","      <td>58.0</td>\n","      <td>74.0</td>\n","      <td>69.0</td>\n","      <td>30.0</td>\n","      <td>21.0</td>\n","      <td>295.0</td>\n","      <td>66.0</td>\n","      <td>53.0</td>\n","      <td>24.0</td>\n","      <td>389.0</td>\n","      <td>83.0</td>\n","      <td>7.0</td>\n","      <td>315.0</td>\n","      <td>55.0</td>\n","      <td>2.0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>3</td>\n","      <td>61.0</td>\n","      <td>40.0</td>\n","      <td>3.0</td>\n","      <td>45.0</td>\n","      <td>75.0</td>\n","      <td>75.0</td>\n","      <td>96.0</td>\n","      <td>44.0</td>\n","      <td>33.0</td>\n","      <td>418.0</td>\n","      <td>96.0</td>\n","      <td>53.0</td>\n","      <td>28.0</td>\n","      <td>463.0</td>\n","      <td>98.0</td>\n","      <td>5.0</td>\n","      <td>291.0</td>\n","      <td>49.0</td>\n","      <td>1.0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>4</td>\n","      <td>27.0</td>\n","      <td>70.0</td>\n","      <td>1.0</td>\n","      <td>20.0</td>\n","      <td>106.0</td>\n","      <td>190.0</td>\n","      <td>127.0</td>\n","      <td>34.0</td>\n","      <td>72.0</td>\n","      <td>659.0</td>\n","      <td>143.0</td>\n","      <td>35.0</td>\n","      <td>33.0</td>\n","      <td>653.0</td>\n","      <td>104.0</td>\n","      <td>44.0</td>\n","      <td>592.0</td>\n","      <td>108.0</td>\n","      <td>1.0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   Unnamed: 0  individual  hinc  psize  ...  invc_car  invt_car  gc_car  mode_chosen\n","0           0        70.0  30.0    4.0  ...       4.0     314.0    52.0          1.0\n","1           1         8.0  15.0    4.0  ...       4.0     351.0    57.0          2.0\n","2           2        62.0  35.0    2.0  ...       7.0     315.0    55.0          2.0\n","3           3        61.0  40.0    3.0  ...       5.0     291.0    49.0          1.0\n","4           4        27.0  70.0    1.0  ...      44.0     592.0   108.0          1.0\n","\n","[5 rows x 20 columns]"]},"metadata":{"tags":[]},"execution_count":3}]},{"cell_type":"markdown","metadata":{"id":"0-vgcX4g9tnF"},"source":["Preprocess data:"]},{"cell_type":"code","metadata":{"id":"Ndh02EBf9tnI","colab":{"base_uri":"https://localhost:8080/","height":68},"executionInfo":{"status":"ok","timestamp":1582409800310,"user_tz":-60,"elapsed":663,"user":{"displayName":"Filipe Rodrigues","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDaWEE0j1oWjHLxVkWKYDXRh8NUBFCgGYF0XZOsvA=s64","userId":"15636531912642599438"}},"outputId":"b77ff3c7-f960-4d6e-8557-70eee73a63e5"},"source":["# separate between features/inputs (X) and target/output variables (y)\n","mat = df.values\n","X = mat[:,2:-1]\n","print(X.shape)\n","y = mat[:,-1].astype(\"int\")\n","print(y.shape)\n","ind = mat[:,1].astype(\"int\")\n","print(ind.shape)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["(394, 17)\n","(394,)\n","(394,)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"BtlsUDCN9tnL"},"source":["### This part is important!\n","\n","This is where we turn our previous 4-class problem into a binary classification problem: car vs non-car"]},{"cell_type":"code","metadata":{"id":"QzkI1pj79tnM"},"source":["# transform to binary problem: car vs non-car\n","y = (y == 4).astype(\"int\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Hg9yXP1R9tnP"},"source":["# standardize input features\n","X_mean = X.mean(axis=0)\n","X_std = X.std(axis=0)\n","X = (X - X_mean) / X_std"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"km1r1Cy59tnT"},"source":["Train/test split:"]},{"cell_type":"code","metadata":{"id":"krO6Vlcl9tnU","colab":{"base_uri":"https://localhost:8080/","height":51},"executionInfo":{"status":"ok","timestamp":1582409805214,"user_tz":-60,"elapsed":780,"user":{"displayName":"Filipe Rodrigues","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDaWEE0j1oWjHLxVkWKYDXRh8NUBFCgGYF0XZOsvA=s64","userId":"15636531912642599438"}},"outputId":"57ed7fd5-b84f-4bbb-897e-d51a08fa9057"},"source":["train_perc = 0.66 # percentage of training data\n","split_point = int(train_perc*len(y))\n","perm = np.random.permutation(len(y))\n","ix_train = perm[:split_point]\n","ix_test = perm[split_point:]\n","X_train = X[ix_train,:]\n","X_test = X[ix_test,:]\n","y_train = y[ix_train]\n","y_test = y[ix_test]\n","print(\"num train: %d\" % len(y_train))\n","print(\"num test: %d\" % len(y_test))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["num train: 260\n","num test: 134\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"pYBgCm7i9tnX"},"source":["Again, for the purpose of comparison, we run the logistic regression method from sklearn. But note that although sklearn has an implementation of logistic regression, it is not a Bayesian approach, nor does it support probit regression or some other variant that you may think is more appropriate for your particular problem. On the other hand, Pyro offers us complete flexibility!"]},{"cell_type":"code","metadata":{"id":"klVJ1GZX9tnY","colab":{"base_uri":"https://localhost:8080/","height":170},"executionInfo":{"status":"ok","timestamp":1582409809467,"user_tz":-60,"elapsed":641,"user":{"displayName":"Filipe Rodrigues","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDaWEE0j1oWjHLxVkWKYDXRh8NUBFCgGYF0XZOsvA=s64","userId":"15636531912642599438"}},"outputId":"95fca951-cfb1-44ea-a60a-c863cc882116"},"source":["# create and fit logistic regression model\n","logreg = linear_model.LogisticRegression(solver='lbfgs', multi_class='auto')\n","logreg.fit(X_train, y_train)\n","\n","# make predictions for test set\n","y_hat = logreg.predict(X_test)\n","print(\"predictions:\", y_hat)\n","print(\"true values:\", y_test)\n","\n","# evaluate prediction accuracy\n","print(\"Accuracy:\", 1.0*np.sum(y_hat == y_test) / len(y_test))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["predictions: [0 0 0 1 1 0 1 0 0 1 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 1 0 1 1 1 0 0 0 0\n"," 0 0 1 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 1 0 1 0 1 0 0 0 0 1 0 0 0 0 0 0 1 1 1\n"," 1 1 0 0 0 0 1 1 0 1 0 1 1 0 0 0 0 0 0 0 0 1 0 0 1 0 0 1 0 0 0 1 1 0 1 0 1\n"," 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1]\n","true values: [1 0 0 1 1 0 0 0 1 1 1 1 0 0 0 0 0 1 0 0 1 0 0 1 0 1 0 0 0 0 1 1 1 1 0 0 0\n"," 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 1 1 0 1 0 1 1 0 0 1 1 1 1 0 0 0 0 0 0 1 0 1\n"," 1 1 0 0 1 0 1 1 0 1 0 0 1 1 0 1 0 0 0 0 1 1 1 1 1 0 1 0 0 0 0 0 1 1 1 0 1\n"," 1 1 0 1 0 0 1 0 0 0 0 1 0 1 0 0 1 0 0 0 0 1 1]\n","Accuracy: 0.7164179104477612\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"WgVxcGpz9tnb"},"source":["Ok, time to implement binary logistic regression in Pyro!\n","\n","Your turn now :-)\n","\n","Note: don't forget to include an explicit intercept parameter $\\alpha$ in the model!"]},{"cell_type":"code","metadata":{"id":"1e4H5ZJFBHHq"},"source":["def model(X, obs=None):\n","    // TODO"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"kPtDsCq7xl9l"},"source":["Let's prepare the data for Pyro by converting it into PyTorch tensors:"]},{"cell_type":"code","metadata":{"id":"DZduqzNghR07"},"source":["# Prepare data for Pyro\n","X_train = torch.tensor(X_train).float()\n","y_train = torch.tensor(y_train).float()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6mbpzhDgxxON"},"source":["Run approximate Bayesian inference using SVI:"]},{"cell_type":"code","metadata":{"id":"BwoCIp9cwciQ","colab":{"base_uri":"https://localhost:8080/","height":527},"executionInfo":{"status":"ok","timestamp":1582411062708,"user_tz":-60,"elapsed":91264,"user":{"displayName":"Filipe Rodrigues","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDaWEE0j1oWjHLxVkWKYDXRh8NUBFCgGYF0XZOsvA=s64","userId":"15636531912642599438"}},"outputId":"2a995507-aee7-4ad3-9cdf-007d683fa19b"},"source":["# Define guide function\n","guide = AutoMultivariateNormal(model)\n","\n","# Reset parameter values\n","pyro.clear_param_store()\n","\n","# Define the number of optimization steps\n","n_steps = 30000\n","\n","# Setup the optimizer\n","adam_params = {\"lr\": 0.001}\n","optimizer = ClippedAdam(adam_params)\n","\n","# Setup the inference algorithm\n","elbo = Trace_ELBO(num_particles=1)\n","svi = SVI(model, guide, optimizer, loss=elbo)\n","\n","# Do gradient steps\n","for step in range(n_steps):\n","    elbo = svi.step(X_train, y_train)\n","    if step % 1000 == 0:\n","        print(\"[%d] ELBO: %.1f\" % (step, elbo))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[0] ELBO: 601.9\n","[1000] ELBO: 304.9\n","[2000] ELBO: 214.1\n","[3000] ELBO: 204.0\n","[4000] ELBO: 196.2\n","[5000] ELBO: 196.3\n","[6000] ELBO: 194.9\n","[7000] ELBO: 193.8\n","[8000] ELBO: 194.2\n","[9000] ELBO: 192.0\n","[10000] ELBO: 186.3\n","[11000] ELBO: 189.9\n","[12000] ELBO: 190.2\n","[13000] ELBO: 190.1\n","[14000] ELBO: 191.8\n","[15000] ELBO: 189.9\n","[16000] ELBO: 190.7\n","[17000] ELBO: 187.5\n","[18000] ELBO: 190.6\n","[19000] ELBO: 187.4\n","[20000] ELBO: 188.1\n","[21000] ELBO: 189.7\n","[22000] ELBO: 188.6\n","[23000] ELBO: 185.0\n","[24000] ELBO: 190.0\n","[25000] ELBO: 189.0\n","[26000] ELBO: 188.2\n","[27000] ELBO: 185.6\n","[28000] ELBO: 188.5\n","[29000] ELBO: 189.3\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"8ScS8Wtqx5-o"},"source":["Upon convergence, we can use the ```Predictive``` class to extract samples from posterior:"]},{"cell_type":"code","metadata":{"id":"BLdm5lVI0mWL"},"source":["from pyro.infer import Predictive\n","\n","predictive = Predictive(model, guide=guide, num_samples=2000,\n","                        return_sites=(\"alpha\", \"beta\"))\n","samples = predictive(X_train, y_train)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"iP7bBPYkeFUz"},"source":["We can now use the inferred posteriors to make predictions for the testset and compute the corresponding accuracy:"]},{"cell_type":"code","metadata":{"id":"nucJ8mtPkQKQ"},"source":["alpha_hat = samples[\"alpha\"].mean(axis=0).detach().numpy()\n","beta_hat = samples[\"beta\"][:,0].mean(axis=0).detach().numpy()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"vxbUB1F5_rD4","colab":{"base_uri":"https://localhost:8080/","height":170},"executionInfo":{"status":"ok","timestamp":1582411064310,"user_tz":-60,"elapsed":87140,"user":{"displayName":"Filipe Rodrigues","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDaWEE0j1oWjHLxVkWKYDXRh8NUBFCgGYF0XZOsvA=s64","userId":"15636531912642599438"}},"outputId":"98b2dbff-9a40-4ccd-a638-d890a69fee0d"},"source":["# make predictions for test set\n","y_hat = alpha_hat + np.dot(X_test, beta_hat)\n","y_hat = (y_hat > 0).astype(np.int)\n","print(\"predictions:\", y_hat)\n","print(\"true values:\", y_test)\n","\n","# evaluate prediction accuracy\n","print(\"Accuracy:\", 1.0*np.sum(y_hat == y_test) / len(y_test))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["predictions: [0 0 0 1 1 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 1 0 1 1 1 0 0 0 0\n"," 0 0 0 1 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 0 1 0 1 1 0 0 0 1 0 0 0 0 0 0 1 1 1\n"," 1 1 0 0 0 0 1 1 0 1 1 1 1 0 0 0 0 0 0 0 0 1 0 0 1 0 0 1 0 0 0 1 1 0 1 0 1\n"," 0 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 1]\n","true values: [1 0 0 1 1 0 0 0 1 1 1 1 0 0 0 0 0 1 0 0 1 0 0 1 0 1 0 0 0 0 1 1 1 1 0 0 0\n"," 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 1 1 0 1 0 1 1 0 0 1 1 1 1 0 0 0 0 0 0 1 0 1\n"," 1 1 0 0 1 0 1 1 0 1 0 0 1 1 0 1 0 0 0 0 1 1 1 1 1 0 1 0 0 0 0 0 1 1 1 0 1\n"," 1 1 0 1 0 0 1 0 0 0 0 1 0 1 0 0 1 0 0 0 0 1 1]\n","Accuracy: 0.7313432835820896\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"kqcThp0w9tny"},"source":["Nice, it seems that we are already doing better than sklearn!"]},{"cell_type":"markdown","metadata":{"id":"KIEAzR9E9tnz"},"source":["Ok, now lets try a **probit regression model in Pyro**.\n","\n","Can you implement it?\n","\n","Hint: you need to find a way to compute the CDF of a standard Gaussian distribution (i.e. N(0,1)) in PyTorch..."]},{"cell_type":"code","metadata":{"id":"0kqqAb7kghCN"},"source":["std_normal = torch.distributions.Normal(0,1)\n","\n","def model(X, obs=None):\n","    // TODO"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"d90t4GeCCCOw"},"source":["Run approximate Bayesian inference using SVI:"]},{"cell_type":"code","metadata":{"id":"x_ALw1szCCOy","colab":{"base_uri":"https://localhost:8080/","height":527},"executionInfo":{"status":"ok","timestamp":1582412137923,"user_tz":-60,"elapsed":102472,"user":{"displayName":"Filipe Rodrigues","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDaWEE0j1oWjHLxVkWKYDXRh8NUBFCgGYF0XZOsvA=s64","userId":"15636531912642599438"}},"outputId":"dd4badd5-c2af-46c7-9b85-712b00b1c67b"},"source":["# Define guide function\n","guide = AutoMultivariateNormal(model)\n","\n","# Reset parameter values\n","pyro.clear_param_store()\n","\n","# Define the number of optimization steps\n","n_steps = 30000\n","\n","# Setup the optimizer\n","adam_params = {\"lr\": 0.0005}\n","optimizer = ClippedAdam(adam_params)\n","\n","# Setup the inference algorithm\n","elbo = Trace_ELBO(num_particles=1)\n","svi = SVI(model, guide, optimizer, loss=elbo)\n","\n","# Do gradient steps\n","for step in range(n_steps):\n","    elbo = svi.step(X_train, y_train)\n","    if step % 1000 == 0:\n","        print(\"[%d] ELBO: %.1f\" % (step, elbo))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[0] ELBO: 1373.6\n","[1000] ELBO: 619.0\n","[2000] ELBO: 415.9\n","[3000] ELBO: 279.5\n","[4000] ELBO: 240.3\n","[5000] ELBO: 213.6\n","[6000] ELBO: 207.9\n","[7000] ELBO: 210.5\n","[8000] ELBO: 205.8\n","[9000] ELBO: 204.6\n","[10000] ELBO: 203.7\n","[11000] ELBO: 200.1\n","[12000] ELBO: 203.0\n","[13000] ELBO: 201.0\n","[14000] ELBO: 199.7\n","[15000] ELBO: 200.1\n","[16000] ELBO: 197.2\n","[17000] ELBO: 200.3\n","[18000] ELBO: 196.0\n","[19000] ELBO: 196.3\n","[20000] ELBO: 194.3\n","[21000] ELBO: 197.9\n","[22000] ELBO: 197.6\n","[23000] ELBO: 197.5\n","[24000] ELBO: 196.1\n","[25000] ELBO: 196.9\n","[26000] ELBO: 197.5\n","[27000] ELBO: 197.2\n","[28000] ELBO: 195.7\n","[29000] ELBO: 197.0\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"odgCy0sMCCO2"},"source":["Upon convergence, we can use the ```Predictive``` class to extract samples from posterior:"]},{"cell_type":"code","metadata":{"id":"1TG1e9SmCCO2"},"source":["from pyro.infer import Predictive\n","\n","predictive = Predictive(model, guide=guide, num_samples=2000,\n","                        return_sites=(\"alpha\", \"beta\"))\n","samples = predictive(X_train, y_train)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0Y-6-GX6CCO4"},"source":["We can now use the inferred posteriors to make predictions for the testset and compute the corresponding accuracy:"]},{"cell_type":"code","metadata":{"id":"_Z50U-dpCCO5"},"source":["alpha_hat = samples[\"alpha\"].mean(axis=0).detach().numpy()\n","beta_hat = samples[\"beta\"][:,0].mean(axis=0).detach().numpy()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"vAoaAuGPCCO7","colab":{"base_uri":"https://localhost:8080/","height":170},"executionInfo":{"status":"ok","timestamp":1582412199624,"user_tz":-60,"elapsed":945,"user":{"displayName":"Filipe Rodrigues","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDaWEE0j1oWjHLxVkWKYDXRh8NUBFCgGYF0XZOsvA=s64","userId":"15636531912642599438"}},"outputId":"a08cd2a5-5d23-4340-c121-56cf3207ae41"},"source":["# make predictions for test set\n","y_hat = alpha_hat + np.dot(X_test, beta_hat)\n","y_hat = (y_hat > 0).astype(np.int)\n","print(\"predictions:\", y_hat)\n","print(\"true values:\", y_test)\n","\n","# evaluate prediction accuracy\n","print(\"Accuracy:\", 1.0*np.sum(y_hat == y_test) / len(y_test))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["predictions: [0 0 0 1 1 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 1 0 1 1 1 0 0 0 0\n"," 0 0 0 1 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 0 1 0 1 1 0 0 0 1 0 0 0 0 0 0 1 1 1\n"," 1 1 0 0 0 0 1 1 0 1 1 1 1 0 0 0 0 0 0 0 0 1 0 0 1 0 0 1 0 0 0 1 1 0 1 0 1\n"," 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1]\n","true values: [1 0 0 1 1 0 0 0 1 1 1 1 0 0 0 0 0 1 0 0 1 0 0 1 0 1 0 0 0 0 1 1 1 1 0 0 0\n"," 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 1 1 0 1 0 1 1 0 0 1 1 1 1 0 0 0 0 0 0 1 0 1\n"," 1 1 0 0 1 0 1 1 0 1 0 0 1 1 0 1 0 0 0 0 1 1 1 1 1 0 1 0 0 0 0 0 1 1 1 0 1\n"," 1 1 0 1 0 0 1 0 0 0 0 1 0 1 0 0 1 0 0 0 0 1 1]\n","Accuracy: 0.7238805970149254\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"collapsed":true,"id":"LfJY0RMm9toM"},"source":["How are your results in comparison to the version with the logistic sigmoid?\n","\n","In some cases, using a probit function instead of the logistic sigmoid can make a significant difference. In other cases, it doesn't... You have to consider what makes more sense to the specific problem that you are trying to solve. Or, we can just try different approaches! That is just fine... Pyro makes it very easy to try all these different variants."]}]}